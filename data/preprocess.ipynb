{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fader_networks/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/fader_networks/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from os import rename\n",
    "from glob import glob\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_dir(data_name, attr_name, target_folders, source_folders, use_shuffle = False) :\n",
    "    images = []\n",
    "    image_formats = ['jpg', 'jpeg', 'png']\n",
    "    \n",
    "    def add_images(images, folders, is_target) :\n",
    "        for folder, n in folders :\n",
    "            n_images = 0\n",
    "            folder_path = os.path.join('./raw_data', folder)\n",
    "            for image_format in image_formats :\n",
    "                temp_images = glob('%s/*.%s'%(folder_path, image_format))\n",
    "                temp_images = temp_images if n is None else temp_images[:n]\n",
    "                temp_attr = [is_target]\n",
    "                temp_attr = temp_attr * len(temp_images)                \n",
    "                images += zip(temp_images, temp_attr)\n",
    "                n_images += len(temp_images)\n",
    "            print('%d images added from %s'%(n_images, folder_path))\n",
    "\n",
    "    add_images(images, source_folders, False)\n",
    "    add_images(images, target_folders, True)\n",
    "    \n",
    "    if use_shuffle :\n",
    "        shuffle(images)\n",
    "    \n",
    "    dir_path = os.path.join('./processed_data', data_name)\n",
    "    image_dir_path = os.path.join(dir_path, 'images')\n",
    "    \n",
    "    assert not os.path.exists(dir_path)\n",
    "    subprocess.Popen(\"mkdir %s\" % dir_path, shell=True).wait()\n",
    "    subprocess.Popen(\"mkdir %s\" % image_dir_path, shell=True).wait()\n",
    "    \n",
    "    f = open(\"%s/list_attr_%s.txt\"%(dir_path, data_name), 'w')\n",
    "    f.write('%d\\n%s\\n'%(len(images), attr_name))\n",
    "    \n",
    "    for idx, (path, attr) in enumerate(images):\n",
    "        file_name = '%06i.jpg' % (idx + 1)\n",
    "        data = '%s %d\\n' % (file_name, 1 if attr else -1)\n",
    "        \n",
    "        shutil.copy(path, os.path.join(image_dir_path, file_name))\n",
    "        f.write(data)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    n_images = len(images)\n",
    "    print(\"Copying %d images completed\"%n_images)\n",
    "    return n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(data_name, n_images):\n",
    "\n",
    "    output_name = '%s_images_%i_%i.pth' % (data_name, IMG_SIZE, IMG_SIZE)\n",
    "    output_path = os.path.join('./processed_data', data_name, output_name)\n",
    "    assert not os.path.isfile(output_path)\n",
    "\n",
    "    print(\"Reading images from %s/ ...\"%(data_name))\n",
    "    \n",
    "    image_dir = os.path.join('./processed_data', data_name, 'images')\n",
    "    \n",
    "    data = np.zeros([n_images, 3, IMG_SIZE, IMG_SIZE], dtype=np.uint8)\n",
    "    for i in range(1, n_images + 1):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        image = mpimg.imread(image_dir + '/%06i.jpg' % i)\n",
    "        assert image.shape == (IMG_SIZE, IMG_SIZE, 3)\n",
    "        data[i - 1, ...] = image.transpose((2, 0, 1))\n",
    "\n",
    "    data = torch.from_numpy(data)\n",
    "    assert data.size() == (n_images, 3, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    print(\"Saving images to %s ...\" % output_path)\n",
    "#     torch.save(data[:20000].clone(), 'images_%i_%i_20000.pth' % (IMG_SIZE, IMG_SIZE))\n",
    "    torch.save(data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_attributes(data_name, n_images):\n",
    "\n",
    "    output_name = '%s_attributes.pth' % (data_name)\n",
    "    output_path = os.path.join('./processed_data', data_name, output_name)\n",
    "    assert not os.path.isfile(output_path)\n",
    "    \n",
    "    file_path = os.path.join('./processed_data', data_name, 'list_attr_%s.txt' % (data_name))\n",
    "    attr_lines = [line.rstrip() for line in open(file_path, 'r')]\n",
    "\n",
    "    assert len(attr_lines) == n_images + 2\n",
    "\n",
    "    attr_keys = attr_lines[1].split()\n",
    "    attributes = {k: np.zeros(n_images, dtype=np.bool) for k in attr_keys}\n",
    "    \n",
    "    for i, line in enumerate(attr_lines[2:]):\n",
    "        image_id = i + 1\n",
    "        split = line.split()\n",
    "#         assert len(split) == 41\n",
    "        assert split[0] == ('%06i.jpg' % image_id)\n",
    "        assert all(x in ['-1', '1'] for x in split[1:])\n",
    "        for j, value in enumerate(split[1:]):\n",
    "            attributes[attr_keys[j]][i] = value == '1'\n",
    "\n",
    "    print(\"Saving attributes to %s ...\" % output_path)\n",
    "    torch.save(attributes, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name : 데이터 이름, 학습 시킬 때 --name 파라미터에 꼭 넣어줘야함\n",
    "# attr : 어트리뷰트 이름, 사람에 따라 다르게 할 필요 없이 하나로 통일 했음\n",
    "data_name = 'angelina_celeba_200k' \n",
    "attr = 'is_target' \n",
    "\n",
    "# [(폴더명, 사진 개 수)] / 사진 개 수에 None을 넣으면 폴더 전체 사진을 추가함\n",
    "# source_folders : 변신 될 사진들 / ex) 셀렙a\n",
    "# target_folders : 변실 시킬 사람 사진들 / ex) 안젤리나 졸리\n",
    "source_folders = [('img_celeba_aligned_128', None)]\n",
    "target_folders = [('Angelina_Jolie_128', None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진 폴더와 attr 리스트를 만든다\n",
    "n_images = make_image_dir(data_name, attr, target_folders, source_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 이미지를 파이토치 형식으로 저장\n",
    "preprocess_images(data_name, n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving attributes to ./processed_data/angelina_celeba_200k/angelina_celeba_200k_attributes.pth ...\n"
     ]
    }
   ],
   "source": [
    "# attr을 파이토치 형식으로 저장\n",
    "preprocess_attributes(data_name, n_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_fader_networks)",
   "language": "python",
   "name": "conda_fader_networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
